{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%reset -f                        # clear all variables from the workspace\n",
    "'generic imports'\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from src import utils   \n",
    "import datetime        \n",
    "\n",
    "'machine learning imports'\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using {}\".format(DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.abspath('../data')\n",
    "\n",
    "# Non-augmented dataset\n",
    "df_train = pd.read_csv(os.path.join(data_dir, 'EdgeIIot_train_100k.csv'), low_memory=False)\n",
    "AUGMENTATION = 'None'\n",
    "\n",
    "# SMOTE augmented dataset\n",
    "# df_train = pd.read_csv(os.path.join(data_dir, 'EdgeIIot_train_100k_SMOTE.csv'), low_memory=False)\n",
    "# AUGMENTATION = 'SMOTE'\n",
    "\n",
    "# SMOTE-NC augmented dataset\n",
    "# df_train = pd.read_csv(os.path.join(data_dir, 'EdgeIIot_train_100k_SMOTE_NC.csv'), low_memory=False)\n",
    "# AUGMENTATION = 'SMOTE-NC'\n",
    "\n",
    "# RealTabFormer augmentation dataset\n",
    "# df_train = pd.read_csv(os.path.join(data_dir, 'EdgeIIot_train_100k_RealTabFormer.csv'), low_memory=False)\n",
    "# AUGMENTATION = 'RealTabFormer'\n",
    "\n",
    "# GReaT augmentation dataset\n",
    "# df_train = pd.read_csv(os.path.join(data_dir, 'EdgeIIot_train_100k_GReaT.csv'), low_memory=False)\n",
    "# AUGMENTATION = 'GReaT'\n",
    "\n",
    "\n",
    "# Test data for all datasets\n",
    "df_test = pd.read_csv(os.path.join(data_dir, 'EdgeIIot_test.csv'), low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns mbtcp.unit_id and mbtcp.trans_id from train and test data    \n",
    "df_train = df_train.drop(['mbtcp.unit_id', 'mbtcp.trans_id'], axis=1)\n",
    "df_test = df_test.drop(['mbtcp.unit_id', 'mbtcp.trans_id'], axis=1)\n",
    "\n",
    "# Creates X_train, y_train\n",
    "X_train = df_train.drop(['Attack_label', 'Attack_type'], axis=1)\n",
    "y_train = df_train['Attack_type']\n",
    "\n",
    "# Creates X_test, y_test\n",
    "X_test = df_test.drop(['Attack_label', 'Attack_type'], axis=1)\n",
    "y_test = df_test['Attack_type']\n",
    "\n",
    "# Extract categorical features\n",
    "categorical_features = X_train.select_dtypes(include=\"object\").columns\n",
    "\n",
    "# Extract indices of categorical features\n",
    "cat_idxs = [X_train.columns.get_loc(col) for col in categorical_features]\n",
    "\n",
    "# Find number of unique values in each categorical column\n",
    "cat_dims = [len(X_train[col].unique()) for col in categorical_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_enc shape: (536515, 53), X_test_enc shape: (381934, 53)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate X_train and X_test to get all possible values for categorical columns\n",
    "X_comb = pd.concat([X_train[categorical_features], X_test[categorical_features]], axis=0)\n",
    "\n",
    "# Apply one-hot encoding (get_dummies)\n",
    "X_comb_enc = pd.get_dummies(X_comb, columns=categorical_features)\n",
    "\n",
    "# Split X_comb_enc into X_train_enc and X_test_enc\n",
    "X_train_enc = X_comb_enc.iloc[:X_train.shape[0], :]\n",
    "X_test_enc = X_comb_enc.iloc[X_train.shape[0]:, :]\n",
    "\n",
    "# # Drop categorical columns from X_train and X_test\n",
    "# X_train = X_train.drop(categorical_features, axis=1)\n",
    "# X_test = X_test.drop(categorical_features, axis=1)\n",
    "\n",
    "# # Concatenate X_train and X_test with X_train_enc and X_test_enc and drop index column\n",
    "# X_train = pd.concat([X_train.reset_index(drop=True), X_train_enc.reset_index(drop=True)], axis=1)\n",
    "# X_test = pd.concat([X_test.reset_index(drop=True), X_test_enc.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Print the shape of X_train and X_test\n",
    "print(f'X_train_enc shape: {X_train_enc.shape}, X_test_enc shape: {X_test_enc.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # AQUI Categorical columns in df_train\n",
    "# categorical_columns = [f for f in features if f in df_train.select_dtypes(include=\"object\").columns]\n",
    "\n",
    "# # Concatenate X_train and X_test\n",
    "# X_comb = pd.concat([X_train[categorical_columns], X_test[categorical_columns]], axis=0)\n",
    "\n",
    "# # Apply one-hot encoding (get_dummies)\n",
    "# X_comb_enc = pd.get_dummies(X_comb, dtype='int8')\n",
    "\n",
    "# # Split back into X_train and X_test\n",
    "# X_train_enc, X_test_enc = train_test_split(\n",
    "#     X_comb_enc, test_size=len(X_test), random_state=42)\n",
    "\n",
    "# # Print the shape of X_train_enc and X_test_enc\n",
    "# print(f'X_train_enc shape: {X_train_enc.shape}, X_test_enc shape: {X_test_enc.shape}')\n",
    "\n",
    "\n",
    "# # converts X_train and y_train to numpy arrays\n",
    "# X_train = df_train[features]\n",
    "# y_train = df_train[\"Attack_type\"]\n",
    "\n",
    "# # converts X_test and y_test to numpy arrays\n",
    "# X_test = df_test[features]\n",
    "# y_test = df_test[\"Attack_type\"]\n",
    "\n",
    "# # size of X_train, y_train  X_test, y_test\n",
    "# print(f'X_train shape: {X_train.shape}, y_train shape: {y_train.shape}')\n",
    "# print(f'X_test shape: {X_test.shape}, y_test shape: {y_test.shape}')\n",
    "# print(cat_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack_type and encoded labels:\n",
      "\n",
      "Backdoor                0\n",
      "DDoS_HTTP               1\n",
      "DDoS_ICMP               2\n",
      "DDoS_TCP                3\n",
      "DDoS_UDP                4\n",
      "Fingerprinting          5\n",
      "MITM                    6\n",
      "Normal                  7\n",
      "Password                8\n",
      "Port_Scanning           9\n",
      "Ransomware              10\n",
      "SQL_injection           11\n",
      "Uploading               12\n",
      "Vulnerability_scanner   13\n",
      "XSS                     14\n"
     ]
    }
   ],
   "source": [
    "# instantiate the label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# fit and encode the training labels\n",
    "y_train = le.fit_transform(y_train)\n",
    "\n",
    "# encode the test labels\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "print('Attack_type and encoded labels:\\n')\n",
    "for i, label in enumerate(le.classes_):\n",
    "    print(f'{label:23s} {i:d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # embedding dimension for each categorical column\n",
    "# cat_emb_dim = 10 \n",
    "\n",
    "# # initialize embedder \n",
    "# cat_embedder = TabNetPretrainer(cat_dims, cat_emb_dim, cat_idxs)\n",
    "\n",
    "# # instantiate TabNetClassifier model\n",
    "# tabnet = TabNetClassifier(device_name = DEVICE,\n",
    "#                           cat_idxs=cat_idxs,\n",
    "#                           cat_dims=cat_dims,\n",
    "#                           cat_emb_dim=cat_emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 8, 25, 31, 39, 40]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from X_train extract indices of categorical columns\n",
    "cat_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ambientes_virtuais_py\\data_augment\\data_augment\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "tabnet = TabNetClassifier(\n",
    "    n_d=64, n_a=64, n_steps=5,\n",
    "    gamma=1.5, n_independent=2, n_shared=2,\n",
    "    cat_idxs=cat_idxs,\n",
    "    cat_dims=cat_dims,\n",
    "    cat_emb_dim=10,\n",
    "    lambda_sparse=1e-4, momentum=0.3, clip_value=2.,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params = {\"gamma\": 0.95, \"step_size\": 20},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ambientes_virtuais_py\\data_augment\\data_augment\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "tabnet.fit(X_train=X_train_enc.values, y_train=y_train,\n",
    "           augmentations=None,\n",
    "           max_epochs=100, \n",
    "           patience=10,\n",
    "           batch_size=1024,\n",
    "           virtual_batch_size=128\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 8, 4, ..., 7, 7, 7])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOTA: NÃO É nËCESSÄRIO APAGAR COLUNAS CATEGÓRICAS DE X_TRAIN E X_TEST SE SE IDENTIFICAREM OS ÍNDICES DAS COLUNAS CATEGÓRICAS E SE SE UTILIZAR O PARÂMETRO cat_idxs=cat_idxs NO MODELO TABNETCLASSIFIER\n",
    "REVER OUTROS NOTEBOOKS!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_filename = tabnet.save_model('checkpoints/tabnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tabnet.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculare and printe a nice board with precision, Recall, F1-score, AUC, Accuracy without classificaiton report\n",
    "print(\"Model Evaluation Metrics\")\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"Accuracy: {}\".format(metrics.accuracy_score(y_test, predictions)))\n",
    "print(\"Precision: {}\".format(metrics.precision_score(y_test, predictions, average='weighted')))\n",
    "print(\"Recall: {}\".format(metrics.recall_score(y_test, predictions, average='weighted')))\n",
    "print(\"F1: {}\".format(metrics.f1_score(y_test, predictions, average='weighted')))\n",
    "print(\"AUC: {}\".format(metrics.roc_auc_score(y_test, predictions, average='weighted')))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = metrics.confusion_matrix(y_test, predictions)\n",
    "# Attack_type: ['DDoS_UDP' 'Password' 'DDoS_TCP' 'Backdoor' 'DDoS_ICMP' 'Port_Scanning'\n",
    "#  'Vulnerability_scanner' 'SQL_injection' 'DDoS_HTTP' 'Uploading' 'XSS'\n",
    "#  'Ransomware' 'MITM' 'Fingerprinting' 'Normal']\n",
    "\n",
    "# conf_mat_df = pd.DataFrame(conf_mat, index = ['DDoS_UDP', 'Password', 'DDoS_TCP', 'Backdoor', 'DDoS_ICMP', 'Port_Scanning', 'Vulnerability_scanner', 'SQL_injection', 'DDoS_HTTP', 'Uploading', 'XSS', 'Ransomware', 'MITM', 'Fingerprinting', 'Normal'], columns = ['DDoS_UDP', 'Password', 'DDoS_TCP', 'Backdoor', 'DDoS_ICMP', 'Port_Scanning', 'Vulnerability_scanner', 'SQL_injection', 'DDoS_HTTP', 'Uploading', 'XSS', 'Ransomware', 'MITM', 'Fingerprinting', 'Normal'])\n",
    "\n",
    "conf_mat_df.index.name = 'Actual'\n",
    "conf_mat_df.columns.name = 'Predicted'\n",
    "print(conf_mat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary for results\n",
    "results = {\n",
    "    \"model\": \"TabNet\",\n",
    "    \"augmentations\": AUGMENTATION,\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"accuracy\": metrics.accuracy_score(y_test, predictions),\n",
    "    \"precision\": metrics.precision_score(y_test, predictions, average='weighted'),\n",
    "    \"recall\": metrics.recall_score(y_test, predictions, average='weighted'),\n",
    "    \"f1\": metrics.f1_score(y_test, predictions, average='weighted'),\n",
    "    \"auc\": metrics.roc_auc_score(y_test, predictions, average='weighted')\n",
    "    }\n",
    "\n",
    "# save results to csv   \n",
    "utils.save_results([results], 'results/TabNet.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_augment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
