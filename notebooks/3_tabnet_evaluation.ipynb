{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%reset -f                        # clear all variables from the workspace\n",
    "'generic imports'\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from src import utils   \n",
    "import datetime        \n",
    "\n",
    "'machine learning imports'\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n",
      "Total GPU memory: 7.8 GB | Current usage: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'CUDA' if torch.cuda.is_available() else 'CPU'\n",
    "print(\"Using {}\".format(DEVICE))\n",
    "\n",
    "# Info on the device available memory\n",
    "if DEVICE == 'CUDA':\n",
    "    \n",
    "    gpu = torch.device('cuda')\n",
    "    total_memory = torch.cuda.get_device_properties(gpu).total_memory / 1024**3\n",
    "    current_memory = torch.cuda.memory_allocated(gpu) / 1024**3\n",
    "\n",
    "    print(f'Total GPU memory: {total_memory:.1f} GB | Current usage: {current_memory:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = os.path.abspath('../data')\n",
    "\n",
    "# Non-augmented dataset\n",
    "df_train = pd.read_csv('../data/EdgeIIot_train_100k.csv', low_memory=False)\n",
    "AUGMENTATION = 'None'\n",
    "\n",
    "# SMOTE augmented dataset\n",
    "# df_train = pd.read_csv(os.path.join(data_dir, 'EdgeIIot_train_100k_SMOTE.csv'), low_memory=False)\n",
    "# AUGMENTATION = 'SMOTE'\n",
    "\n",
    "# SMOTE-NC augmented dataset\n",
    "# df_train = pd.read_csv(os.path.join(data_dir, 'EdgeIIot_train_100k_SMOTE_NC.csv'), low_memory=False)\n",
    "# AUGMENTATION = 'SMOTE-NC'\n",
    "\n",
    "# RealTabFormer augmentation dataset\n",
    "# df_train = pd.read_csv(os.path.join(data_dir, 'EdgeIIot_train_100k_RealTabFormer.csv'), low_memory=False)\n",
    "# AUGMENTATION = 'RealTabFormer'\n",
    "\n",
    "# GReaT augmentation dataset\n",
    "# df_train = pd.read_csv(os.path.join(data_dir, 'EdgeIIot_train_100k_GReaT.csv'), low_memory=False)\n",
    "# AUGMENTATION = 'GReaT'\n",
    "\n",
    "\n",
    "# Test data for all datasets\n",
    "df_test = pd.read_csv('../data/EdgeIIot_test.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns mbtcp.unit_id and mbtcp.trans_id from train and test data    \n",
    "df_train = df_train.drop(['mbtcp.unit_id', 'mbtcp.trans_id'], axis=1)\n",
    "df_test = df_test.drop(['mbtcp.unit_id', 'mbtcp.trans_id'], axis=1)\n",
    "\n",
    "# Creates X_train, y_train\n",
    "X_train = df_train.drop(['Attack_label', 'Attack_type'], axis=1)\n",
    "y_train = df_train['Attack_type']\n",
    "\n",
    "# Creates X_test, y_test\n",
    "X_test = df_test.drop(['Attack_label', 'Attack_type'], axis=1)\n",
    "y_test = df_test['Attack_type']\n",
    "\n",
    "# Extract categorical features\n",
    "categorical_features = X_train.select_dtypes(include=\"object\").columns\n",
    "\n",
    "# Extract indices of categorical features\n",
    "cat_idxs = [X_train.columns.get_loc(col) for col in categorical_features]\n",
    "\n",
    "# Find number of unique values in each categorical column\n",
    "cat_dims = [len(X_train[col].unique()) for col in categorical_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_enc shape: (536515, 53), X_test_enc shape: (381934, 53)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate X_train and X_test to get all possible values for categorical columns\n",
    "X_comb = pd.concat([X_train[categorical_features], X_test[categorical_features]], axis=0)\n",
    "\n",
    "# Apply one-hot encoding (get_dummies)\n",
    "X_comb_enc = pd.get_dummies(X_comb, columns=categorical_features)\n",
    "\n",
    "# Split X_comb_enc into X_train_enc and X_test_enc\n",
    "X_train_enc = X_comb_enc.iloc[:X_train.shape[0], :]\n",
    "X_test_enc = X_comb_enc.iloc[X_train.shape[0]:, :]\n",
    "\n",
    "# # Drop categorical columns from X_train and X_test\n",
    "# X_train = X_train.drop(categorical_features, axis=1)\n",
    "# X_test = X_test.drop(categorical_features, axis=1)\n",
    "\n",
    "# # Concatenate X_train and X_test with X_train_enc and X_test_enc and drop index column\n",
    "# X_train = pd.concat([X_train.reset_index(drop=True), X_train_enc.reset_index(drop=True)], axis=1)\n",
    "# X_test = pd.concat([X_test.reset_index(drop=True), X_test_enc.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Print the shape of X_train and X_test\n",
    "print(f'X_train_enc shape: {X_train_enc.shape}, X_test_enc shape: {X_test_enc.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # AQUI Categorical columns in df_train\n",
    "# categorical_columns = [f for f in features if f in df_train.select_dtypes(include=\"object\").columns]\n",
    "\n",
    "# # Concatenate X_train and X_test\n",
    "# X_comb = pd.concat([X_train[categorical_columns], X_test[categorical_columns]], axis=0)\n",
    "\n",
    "# # Apply one-hot encoding (get_dummies)\n",
    "# X_comb_enc = pd.get_dummies(X_comb, dtype='int8')\n",
    "\n",
    "# # Split back into X_train and X_test\n",
    "# X_train_enc, X_test_enc = train_test_split(\n",
    "#     X_comb_enc, test_size=len(X_test), random_state=42)\n",
    "\n",
    "# # Print the shape of X_train_enc and X_test_enc\n",
    "# print(f'X_train_enc shape: {X_train_enc.shape}, X_test_enc shape: {X_test_enc.shape}')\n",
    "\n",
    "\n",
    "# # converts X_train and y_train to numpy arrays\n",
    "# X_train = df_train[features]\n",
    "# y_train = df_train[\"Attack_type\"]\n",
    "\n",
    "# # converts X_test and y_test to numpy arrays\n",
    "# X_test = df_test[features]\n",
    "# y_test = df_test[\"Attack_type\"]\n",
    "\n",
    "# # size of X_train, y_train  X_test, y_test\n",
    "# print(f'X_train shape: {X_train.shape}, y_train shape: {y_train.shape}')\n",
    "# print(f'X_test shape: {X_test.shape}, y_test shape: {y_test.shape}')\n",
    "# print(cat_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack_type and encoded labels:\n",
      "\n",
      "Backdoor                0\n",
      "DDoS_HTTP               1\n",
      "DDoS_ICMP               2\n",
      "DDoS_TCP                3\n",
      "DDoS_UDP                4\n",
      "Fingerprinting          5\n",
      "MITM                    6\n",
      "Normal                  7\n",
      "Password                8\n",
      "Port_Scanning           9\n",
      "Ransomware              10\n",
      "SQL_injection           11\n",
      "Uploading               12\n",
      "Vulnerability_scanner   13\n",
      "XSS                     14\n"
     ]
    }
   ],
   "source": [
    "# instantiate the label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# fit and encode the training labels\n",
    "y_train = le.fit_transform(y_train)\n",
    "\n",
    "# encode the test labels\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "print('Attack_type and encoded labels:\\n')\n",
    "for i, label in enumerate(le.classes_):\n",
    "    print(f'{label:23s} {i:d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # embedding dimension for each categorical column\n",
    "# cat_emb_dim = 10 \n",
    "\n",
    "# # initialize embedder \n",
    "# cat_embedder = TabNetPretrainer(cat_dims, cat_emb_dim, cat_idxs)\n",
    "\n",
    "# # instantiate TabNetClassifier model\n",
    "# tabnet = TabNetClassifier(device_name = DEVICE,\n",
    "#                           cat_idxs=cat_idxs,\n",
    "#                           cat_dims=cat_dims,\n",
    "#                           cat_emb_dim=cat_emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiagociic/Projectos/inovmineral/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "# tabnet = TabNetClassifier(n_d=64, \n",
    "#                           n_a=64, \n",
    "#                           n_steps=5,\n",
    "#                           gamma=1.5,\n",
    "#                           cat_idxs=cat_idxs,\n",
    "#                           cat_dims=cat_dims,\n",
    "#                           cat_emb_dim=10,\n",
    "#                           lambda_sparse=1e-4, \n",
    "#                           momentum=0.3, \n",
    "#                           clip_value=2.,\n",
    "#                           optimizer_params=dict(lr=2e-2),\n",
    "#                           scheduler_params = {\"gamma\": 0.95, \"step_size\": 20},\n",
    "#                           scheduler_fn=torch.optim.lr_scheduler.StepLR, \n",
    "#                           )\n",
    "\n",
    "# pytorch_tabnet default parameters default parameters except for cat_emb_dim, which is set to 10\n",
    "tabnet = TabNetClassifier(cat_idxs=cat_idxs,\n",
    "                          cat_dims=cat_dims,\n",
    "                          cat_emb_dim=10,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabModel(n_d: int = 8,\n",
    "#         n_a: int = 8, \n",
    "#         n_steps: int = 3, \n",
    "#         gamma: float = 1.3, \n",
    "#         cat_idxs: List[int] = <factory>, \n",
    "#         cat_dims: List[int] = <factory>, \n",
    "#         cat_emb_dim: int = 1, \n",
    "#         n_independent: int = 2, \n",
    "#         n_shared: int = 2, \n",
    "#         epsilon: float = 1e-15, \n",
    "#         momentum: float = 0.02, \n",
    "#         lambda_sparse: float = 0.001, \n",
    "#         seed: int = 0, \n",
    "#         clip_value: int = 1, \n",
    "#         verbose: int = 1, \n",
    "#         optimizer_fn: Any = <class 'torch.optim.adam.Adam'>, \n",
    "#         optimizer_params: Dict = <factory>, \n",
    "#         scheduler_fn: Any = None, \n",
    "#         scheduler_params: Dict = <factory>, \n",
    "#         mask_type: str = 'sparsemax', \n",
    "#         input_dim: int = None, \n",
    "#         output_dim: int = None, \n",
    "#         device_name: str = 'auto', \n",
    "#         n_shared_decoder: int = 1, \n",
    "#         n_indep_decoder: int = 1, \n",
    "#         grouped_features: List[List[int]] = <factory>\n",
    "#         )\n",
    "\n",
    "# fit(X_train, \n",
    "#     y_train, \n",
    "#     eval_set=None, \n",
    "#     eval_name=None, \n",
    "#     eval_metric=None, \n",
    "#     loss_fn=None, \n",
    "#     weights=0, \n",
    "#     max_epochs=100, \n",
    "#     patience=10, \n",
    "#     batch_size=1024, \n",
    "#     virtual_batch_size=128, \n",
    "#     num_workers=0, \n",
    "#     drop_last=True, \n",
    "#     callbacks=None, \n",
    "#     pin_memory=True, \n",
    "#     from_unsupervised=None, \n",
    "#     warm_start=False, \n",
    "#     augmentations=None, \n",
    "#     compute_importance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiagociic/Projectos/inovmineral/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.15238 |  0:00:12s\n",
      "epoch 1  | loss: 1.03713 |  0:00:25s\n",
      "epoch 2  | loss: 1.03388 |  0:00:38s\n",
      "epoch 3  | loss: 1.03318 |  0:00:51s\n",
      "epoch 4  | loss: 1.03445 |  0:01:05s\n",
      "epoch 5  | loss: 1.03777 |  0:01:17s\n",
      "epoch 6  | loss: 1.03337 |  0:01:30s\n",
      "epoch 7  | loss: 1.0328  |  0:01:43s\n",
      "epoch 8  | loss: 1.03237 |  0:01:56s\n",
      "epoch 9  | loss: 1.03239 |  0:02:08s\n",
      "epoch 10 | loss: 1.03226 |  0:02:21s\n",
      "epoch 11 | loss: 1.03209 |  0:02:34s\n",
      "epoch 12 | loss: 1.03208 |  0:02:47s\n",
      "epoch 13 | loss: 1.03192 |  0:03:00s\n",
      "epoch 14 | loss: 1.03207 |  0:03:13s\n",
      "epoch 15 | loss: 1.04428 |  0:03:25s\n",
      "epoch 16 | loss: 1.04026 |  0:03:39s\n",
      "epoch 17 | loss: 1.04004 |  0:03:52s\n",
      "epoch 18 | loss: 1.0399  |  0:04:05s\n",
      "epoch 19 | loss: 1.03993 |  0:04:18s\n",
      "epoch 20 | loss: 1.03988 |  0:04:32s\n",
      "epoch 21 | loss: 1.03993 |  0:04:45s\n",
      "epoch 22 | loss: 1.04721 |  0:04:58s\n",
      "epoch 23 | loss: 1.05987 |  0:05:11s\n",
      "epoch 24 | loss: 1.05886 |  0:05:24s\n",
      "epoch 25 | loss: 1.05891 |  0:05:37s\n",
      "epoch 26 | loss: 1.05888 |  0:05:50s\n",
      "epoch 27 | loss: 1.05884 |  0:06:02s\n",
      "epoch 28 | loss: 1.05885 |  0:06:15s\n",
      "epoch 29 | loss: 1.05883 |  0:06:28s\n"
     ]
    }
   ],
   "source": [
    "# pytorch_tabnet default parameters\n",
    "tabnet.fit(X_train=X_train_enc.values, \n",
    "           y_train=y_train,\n",
    "           augmentations=None,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 8, 4, ..., 7, 7, 7])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTA: NÃO É nËCESSÄRIO APAGAR COLUNAS CATEGÓRICAS DE X_TRAIN E X_TEST SE SE IDENTIFICAREM OS ÍNDICES DAS COLUNAS CATEGÓRICAS E SE SE UTILIZAR O PARÂMETRO cat_idxs=cat_idxs NO MODELO TABNETCLASSIFIER\n",
    "# REVER OUTROS NOTEBOOKS!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_filename = tabnet.save_model('checkpoints/tabnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tabnet.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "precision = metrics.precision_score(y_test, predictions, average='weighted', zero_division=1)\n",
    "recall = metrics.recall_score(y_test, predictions, average='weighted')\n",
    "f1_score = metrics.f1_score(y_test, predictions, average='weighted')\n",
    "\n",
    "print(\"Model Evaluation Metrics\")\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"Precision (Weighted): {:.2f}\".format(precision))\n",
    "print(\"Recall (Weighted): {:.2f}\".format(recall))\n",
    "print(\"F1(Weighted): {:.2f}\".format(f1_score))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Metrics Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary for results\n",
    "results = {\n",
    "    \"model\": \"tabnet\",\n",
    "    \"augmentations\": AUGMENTATION,\n",
    "    \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1_score\n",
    "    }\n",
    "\n",
    "# save results to csv   \n",
    "utils.save_results_to_csv([results], '../results/metrics/tabnet.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = metrics.confusion_matrix(y_test, predictions)\n",
    "\n",
    "attack_labels = ['Backdoor', 'DDoS_HTTP', 'DDoS_ICMP', 'DDoS_TCP', 'DDoS_UDP', \n",
    "'Fingerprinting', 'MITM', 'Normal', 'Password', 'Port_Scanning', 'Ransomware', \n",
    "'SQL_injection', 'Uploading', 'Vulnerability_scanner', 'XSS']\n",
    "\n",
    "# Create a dataframe from the confusion matrix\n",
    "conf_mat_df = pd.DataFrame(conf_mat, \n",
    "                            index = attack_labels, \n",
    "                            columns = attack_labels)\n",
    "conf_mat_df.index.name = 'Actual'\n",
    "conf_mat_df.columns.name = 'Predicted'\n",
    "\n",
    "# Save the confusion matrix\n",
    "conf_mat_df.to_csv(f\"../results/conf_matrix/{results['model']}_{results['augmentations']}.csv\")\n",
    "conf_mat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary for results\n",
    "results = {\n",
    "    \"model\": \"TabNet\",\n",
    "    \"augmentations\": AUGMENTATION,\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"accuracy\": metrics.accuracy_score(y_test, predictions),\n",
    "    \"precision\": metrics.precision_score(y_test, predictions, average='weighted'),\n",
    "    \"recall\": metrics.recall_score(y_test, predictions, average='weighted'),\n",
    "    \"f1\": metrics.f1_score(y_test, predictions, average='weighted'),\n",
    "    \"auc\": metrics.roc_auc_score(y_test, predictions, average='weighted')\n",
    "    }\n",
    "\n",
    "# save results to csv   \n",
    "utils.save_results([results], 'results/TabNet.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_augment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
